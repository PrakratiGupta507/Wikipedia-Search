{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Model_Comparison_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4g96ip5LN--",
        "outputId": "e1a74067-1d95-4e7d-e9f2-ae7b181cf52a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQFa6H-GmbzT",
        "outputId": "ba0e8e0f-5035-471f-ebc9-72aa09b45e13"
      },
      "source": [
        "import codecs                     #used\n",
        "import string\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import ast \n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R9pfQCim2SF"
      },
      "source": [
        "def preprocessing(texts):                   #text preprocessing                   #used\n",
        "    #word_tokenize\n",
        "#     print(texts)\n",
        "#     print()\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          texts)\n",
        "#     print(letters_only)\n",
        "#     print()\n",
        "    word_tokens = word_tokenize(letters_only)\n",
        "#     print(word_tokens)\n",
        "#     print()\n",
        "    #Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_lemm = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stopwords.words('english')]\n",
        "#     print(word_lemm)\n",
        "#     print()\n",
        "    return word_lemm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aVSqIs9m9wZ"
      },
      "source": [
        "def preprocessing_data(files):                        #file preprocessing like whom we will append in the final data                #used\n",
        "    preprocessedData = {}\n",
        "    data = [] #to store the data of the files\n",
        "    for file in files:\n",
        "        if(file != 'FARNON' and file != 'SRE' and file != 'whgdsreg' and file != \".header\" and file != \".musings\" and file != \".descs\" and file != \"index.html\"):\n",
        "            with open('/content/drive/MyDrive/IR/2/stories/'+file, mode='r',errors=\"ignore\",encoding = \"ISO-8859-1\") as f:\n",
        "                data.append(f.read().lower())\n",
        "#                 preprocessedData[file] = preprocessing(f.read())\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o055cF_nBoQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjacfYTInEm_"
      },
      "source": [
        "f = open('/content/drive/MyDrive/IRE/preprocessedData.json')             #loading posting list\n",
        "preprocessedData = json.load(f)\n",
        "\n",
        "f = open('/content/drive/MyDrive/IRE/postinglist.json')             #loading posting list\n",
        "postings = json.load(f)\n",
        "\n",
        "f1 = open('/content/drive/MyDrive/IRE/invertedIndexing.json')              #loading the inverted indexing\n",
        "inverted_indexing = json.load(f1)\n",
        "data1 = inverted_indexing.copy()\n",
        "\n",
        "f2 = open('/content/drive/MyDrive/IRE/title_indexing.json')                         #loading title indexing\n",
        "title_indexing = json.load(f2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMPZPHRks-dX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMOhSkdrnLgQ"
      },
      "source": [
        "# # {word : {key1:list of occurence of word in key1, key2:list of occurence of word in key2}, word2:{key1:list of occurence of word1 in key1}}                   \n",
        "\n",
        "# new_postings = {}                                  #don't run it\n",
        "# counting = 1\n",
        "\n",
        "# for item in inverted_indexing.keys():       # item is the term\n",
        "#   k = inverted_indexing[item]       # k is term id for term \n",
        "#   k = str(k)\n",
        "#   doc_list = postings[k]    # document list in which item is present  \n",
        "#   new_postings[item] = {}\n",
        "#   for doc in doc_list:\n",
        "#     ind_list = all_index(preprocessedData[doc],item)\n",
        "#     new_postings[item][doc] = ind_list\n",
        "#   counting += 1  \n",
        "\n",
        "# with open('/content/drive/MyDrive/IRE/Assignment 2/Que 1/word_posting_positional.json', 'w') as fp:          #already run\n",
        "#     json.dump(new_postings, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JTshEuNCWIq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBPfCCO-yORu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d4fc86-a32c-491a-a1ff-3bf71665c3df"
      },
      "source": [
        "# {word : {key1:list of occurence of word in key1, key2:list of occurence of word in key2}, word2:{key1:list of occurence of word1 in key1}}                   \n",
        "start=time.time()\n",
        "\n",
        "new_postings_new = {}                                  \n",
        "counting = 1\n",
        "\n",
        "for item in inverted_indexing.keys():       # item is the term\n",
        "  k = inverted_indexing[item]       # k is term id for term \n",
        "  k = str(k)\n",
        "  doc_list = postings[k]    # document list in which item is present  \n",
        "  new_postings_new[item] = doc_list\n",
        "  counting += 1  \n",
        "\n",
        "end=time.time()\n",
        "print(\"Running Time for the indexing  : \",(end-start)*1000,\" ms\")\n",
        "time1 = (end-start)*1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Time for the indexing  :  62.83140182495117  ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c11DSaGdd5pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b94757b-7f43-4a50-be7c-48bd6a48d612"
      },
      "source": [
        "start=time.time()                  #creating indexing for baseline         #run it for indexing time\n",
        "\n",
        "\n",
        "doc_final = []            #creating final document list with scriptle and                    #used\n",
        "#val = title_indexing.keys()\n",
        "for i in title_indexing.keys():\n",
        "  doc_final.append(i)\n",
        "\n",
        "# {word : {documents in sparse form }, word2:{documents in sparse form }}                            #used\n",
        "\n",
        "new_sparse_indexing = {}\n",
        "counting = 1\n",
        "for item in inverted_indexing.keys():       # item is the term\n",
        "  k = inverted_indexing[item]       # k is term id for term \n",
        "  k = str(k)\n",
        "  doc_list = postings[k]    # document list in which item is present  \n",
        "  #val = np.zeros((len(title_indexing.keys())),dtype=np.uint8)\n",
        "  val = np.zeros(len(title_indexing.keys()))\n",
        "  doc_list = list(doc_list)\n",
        "  for j in doc_list:\n",
        "    index1 = doc_final.index(j)\n",
        "    #print(index1)\n",
        "    val[index1] = 1\n",
        "  new_sparse_indexing[item] = val\n",
        "  counting = counting + 1\n",
        "  #print(counting)  \n",
        "\n",
        "\n",
        "\n",
        "end=time.time()\n",
        "print(\"Running Time for the indexing  : \",(end-start)*1000,\" ms\")\n",
        "time2 = (end-start)*1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Time for the indexing  :  1950.8910179138184  ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3VH-wlAi2Pc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyeth1zStLEA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbqZ-A2K7xOs"
      },
      "source": [
        "**Query Part**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afDGS1aZY2R5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1e5687-aa29-4d5e-b56e-39f80ecb9ce5"
      },
      "source": [
        "import codecs                    \n",
        "import string\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import ast \n",
        "import time\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKBTpzA5Y9I2"
      },
      "source": [
        "def preprocessing(texts):                   #text preprocessing                   \n",
        "    #word_tokenize\n",
        "#     print(texts)\n",
        "#     print()\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          texts)\n",
        "#     print(letters_only)\n",
        "#     print()\n",
        "    word_tokens = word_tokenize(letters_only)\n",
        "#     print(word_tokens)\n",
        "#     print()\n",
        "    #Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_lemm = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stopwords.words('english')]\n",
        "#     print(word_lemm)\n",
        "#     print()\n",
        "    return word_lemm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4TMQt5bZDHw"
      },
      "source": [
        "f = open('/content/drive/MyDrive/IRE/postinglist.json')             #loading posting list\n",
        "postings = json.load(f)\n",
        "\n",
        "f1 = open('/content/drive/MyDrive/IRE/invertedIndexing.json')              #loading the inverted indexing\n",
        "inverted_indexing = json.load(f1)\n",
        "data1 = inverted_indexing.copy()\n",
        "\n",
        "f2 = open('/content/drive/MyDrive/IRE/title_indexing.json')                         #loading title indexing\n",
        "title_indexing = json.load(f2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei_TAJnXKXfv"
      },
      "source": [
        "# f =  open('/content/drive/MyDrive/IRE/Assignment 2/Que 1/word_posting_positional.json')       #loading postion posting list\n",
        "# word_posting_positional = json.load(f)\n",
        "\n",
        "#word_posting_positional = new_postings.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leWDDKyyI-so"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xY2EynX6o60"
      },
      "source": [
        "**Our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wGvcUI3vox2",
        "outputId": "415b3a25-7083-4e41-d5ee-72bf99b2e515"
      },
      "source": [
        "input1 = input(\"Enter the Query\")                 # input query       \n",
        "start=time.time()\n",
        "\n",
        "print(\"Input Query is :\",input1)\n",
        "query = preprocessing(input1.lower())          #preprocessing\n",
        "print(\"Tokens are:\",query)                      #getting tokens after preprocessing\n",
        "\n",
        "res = []\n",
        "final_res = []\n",
        "i = 0\n",
        "j = 0\n",
        "\n",
        "if len(query) == 0:\n",
        "  print('No query terms are present so No documents will be fetched')\n",
        "\n",
        "elif len(query) == 1:\n",
        "    term1 = query[0]\n",
        "    #list1 = word_posting_positional[term1].keys()\n",
        "    list1 = new_postings_new[term1]\n",
        "    print('No of documents present :',len(list1))\n",
        "    print(\"Documents are :\", list1)\n",
        "\n",
        "\n",
        "else:\n",
        "  while i < len(query):  \n",
        "    if j == 0:\n",
        "      j = 1\n",
        "      i = i+2\n",
        "      term1 = query[0]\n",
        "      term2 = query[1]\n",
        "      #print(term1)\n",
        "      #list1 = word_posting_positional[term1].keys()\n",
        "      list1 = new_postings_new[term1]\n",
        "      list2 = new_postings_new[term2]\n",
        "    \n",
        "      #list2 = word_posting_positional[term2].keys()\n",
        "\n",
        "      if len(list1) >= len(list2):\n",
        "          final_res = list(set(list1).intersection(set(list2)))\n",
        "\n",
        "      else:\n",
        "          final_res = list(set(list2).intersection(set(list1)))\n",
        "    else: \n",
        "          \n",
        "      j = 1\n",
        "      term = query[i]\n",
        "      #list1 = word_posting_positional[term]\n",
        "      list1 = new_postings_new[term]\n",
        "\n",
        "      if len(list1) <= len(final_res):\n",
        "          final_res = list(set(final_res).intersection(set(list1)))\n",
        "\n",
        "      else:\n",
        "            final_res = list(set(list1).intersection(set(final_res)))\n",
        "\n",
        "      i = i+1\n",
        "\n",
        "  print(\"No of Documents in which all query terms present \",len(final_res))       \n",
        "  print(\"Documents in which all query terms present \",final_res)\n",
        "\n",
        "\n",
        "end=time.time()\n",
        "print(\"Running Time for the quesry  : \",(end-start)*1000,\" ms\")\n",
        "\n",
        "print(\"Running Time for the quesry and Indexing : \",(end-start)*1000 + time1,\" ms\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Querytelephone\n",
            "Input Query is : telephone\n",
            "Tokens are: ['telephone']\n",
            "No of documents present : 36\n",
            "Documents are : {'hitch2.txt': 32, 'bulphrek.txt': 24, 'corcor.hum': 24, 'cybersla.txt': 20, 'archive': 16, 'taxnovel.txt': 16, '100west.txt': 12, 'bureau.txt': 12, 'hitch3.txt': 12, 'telefone.txt': 12, 'bulzork1.txt': 8, 'crazy.hum': 8, 'jaynejob.asc': 8, 'aircon.txt': 4, 'breaks1.asc': 4, 'excerpt.hum': 4, 'excerpt.txt': 4, 'fgoose.txt': 4, 'fic5': 4, 'ghost': 4, 'graymare.txt': 4, 'hotline4.txt': 4, 'korea.s': 4, 'long1-3.txt': 4, 'paink-ws.txt': 4, 'progx': 4, 'radar_ra.txt': 4, 'running.txt': 4, 'sick-kid.txt': 4, 'social.vikings': 4, 'socialvikings.txt': 4, 'space.txt': 4, 'sqzply.txt': 4, 'szechuan': 4, 'timem.hac': 4, 'zombies.txt': 4}\n",
            "Running Time for the quesry  :  1.3663768768310547  ms\n",
            "Running Time for the quesry and Indexing :  64.19777870178223  ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNXCyXgMzEUX"
      },
      "source": [
        "**Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydwnZIIMA_Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbebf255-88fd-4299-bed9-f130b92003cd"
      },
      "source": [
        "input1 = input(\"Enter the Query\")                 # input query  \n",
        "start=time.time()        \n",
        "print(\"Input Query is :\",input1)\n",
        "query = preprocessing(input1.lower())          #preprocessing\n",
        "print(\"Tokens are:\",query)                      #getting tokens after preprocessing\n",
        "\n",
        "res = []\n",
        "final_res = []\n",
        "i = 0\n",
        "j = 0\n",
        "\n",
        "if len(query) == 0:\n",
        "  print('No query terms are present so No documents will be fetched')\n",
        "\n",
        "elif len(query) == 1:\n",
        "    term1 = query[0]\n",
        "    #list1 = word_posting_positional[term1].keys()\n",
        "    #list1 = new_sparse_indexing[term1]\n",
        "    val5 = np.where(new_sparse_indexing[term1]==1)\n",
        "    ans = val5[0]\n",
        "    #print(ans)\n",
        "    print('No of documents present :',len(ans))\n",
        "    final_ans = []\n",
        "    for i in range(len(ans)):\n",
        "      j = ans[i]\n",
        "      #print(doc_final[j])\n",
        "      final_ans.append(doc_final[j])\n",
        "\n",
        "    print(\"Documents are :\", final_ans)\n",
        "\n",
        "\n",
        "else:\n",
        "  while i < len(query):  \n",
        "    if j == 0:\n",
        "      j = 1\n",
        "      i = i+2\n",
        "      term1 = query[0]\n",
        "      term2 = query[1]\n",
        "      #print(term1)\n",
        "      #list1 = word_posting_positional[term1].keys()\n",
        "      list1 = np.where(new_sparse_indexing[term1]==1)[0]\n",
        "\n",
        "    \n",
        "      #list2 = word_posting_positional[term2].keys()\n",
        "      list2 = np.where(new_sparse_indexing[term2]==1)[0]\n",
        "      #list2 = list(list2)\n",
        "\n",
        "\n",
        "      if len(list1) >= len(list2):\n",
        "          final_res = list(set(list1).intersection(set(list2)))\n",
        "\n",
        "      else:\n",
        "          final_res = list(set(list2).intersection(set(list1)))\n",
        "    else: \n",
        "          \n",
        "      j = 1\n",
        "      term = query[i]\n",
        "      #list1 = word_posting_positional[term]\n",
        "      list1 = np.where(new_sparse_indexing[term]==1)[0]\n",
        "      #list1 = list(list1)\n",
        "\n",
        "      if len(list1) <= len(final_res):\n",
        "          final_res = list(set(final_res).intersection(set(list1)))\n",
        "\n",
        "      else:\n",
        "            final_res = list(set(list1).intersection(set(final_res)))\n",
        "\n",
        "      i = i+1\n",
        "\n",
        "  print(\"No of Documents in which all query terms present \",len(final_res))    \n",
        "  final_ans = []\n",
        "  for i in range(len(final_res)):\n",
        "      j = final_res[i]\n",
        "      #print(doc_final[j])\n",
        "      final_ans.append(doc_final[j])\n",
        "\n",
        "  #print(\"Documents are :\", final_ans)   \n",
        "  print(\"Documents in which all query terms present \",final_ans)\n",
        "\n",
        "  #count = 0\n",
        "end=time.time()\n",
        "print(\"Running Time for the quesry  : \",(end-start)*1000,\" ms\") \n",
        "print(\"Running Time for the quesry and Indexing : \",(end-start)*1000 + time2,\" ms\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Querytelephone\n",
            "Input Query is : telephone\n",
            "Tokens are: ['telephone']\n",
            "No of documents present : 36\n",
            "Documents are : ['100west.txt', 'aircon.txt', 'archive', 'breaks1.asc', 'bulphrek.txt', 'bulzork1.txt', 'bureau.txt', 'corcor.hum', 'crazy.hum', 'cybersla.txt', 'excerpt.hum', 'excerpt.txt', 'fgoose.txt', 'fic5', 'ghost', 'graymare.txt', 'hitch2.txt', 'hitch3.txt', 'hotline4.txt', 'jaynejob.asc', 'korea.s', 'long1-3.txt', 'paink-ws.txt', 'progx', 'radar_ra.txt', 'running.txt', 'sick-kid.txt', 'social.vikings', 'socialvikings.txt', 'space.txt', 'sqzply.txt', 'szechuan', 'taxnovel.txt', 'telefone.txt', 'timem.hac', 'zombies.txt']\n",
            "Running Time for the quesry  :  3.256559371948242  ms\n",
            "Running Time for the quesry and Indexing :  1954.1475772857666  ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-9r3l678FbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}